{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ef6227e",
   "metadata": {},
   "source": [
    "*UE Learning from User-generated Data, CP MMS, JKU Linz 2025*\n",
    "# Exercise 4: Evaluation\n",
    "\n",
    "Evaluating a recommender system using offline metrics is crucial to ensuring its quality before deployment. The choice of evaluation metrics is typically guided by the specific application scenario of the recommendation system. Therefore, it is essential to understand how each metric is calculated and what it measures.\n",
    "\n",
    "In this exercise we evaluate accuracy of the three different RecSys we already implemented (``TopPop``, ``ItemKNN`` and ``SVD``). The first two tasks are about predictive quality metrics, precisely about ``Precision@K`` and ``Recall@K`` respectively. Afterwards, we take a look into ranking quality metrics, especially into ``DCG`` and ``nDCG``. At the end all three recommender systems are evaluated based on these metrics. \n",
    "\n",
    "The implementations for the three recommender systems are provided in a file ``rec.py`` and are imported later in the notebook.\n",
    "\n",
    "Make sure to rename the notebook according to the convention:\n",
    "\n",
    "LUD25_ex04_k<font color='red'><Matr. Number\\></font>_<font color='red'><Surname-Name\\></font>.ipynb\n",
    "\n",
    "for example:\n",
    "\n",
    "LUD25_ex04_k000007_Bond_James.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fa9f03",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Please consult the lecture slides and the presentation from UE Session 4 for a recap.\n",
    "\n",
    "|Metric|Range|Selection criteria|Limitation|\n",
    "|------|-------------------------------|---------|----------|\n",
    "|Precision|$\\geq 0$ and $\\leq 1$|The closer to $1$ the better.|Only for hits in recommendations. Rank-agnostic.                                                        |\n",
    "|Recall|$\\geq 0$ and $\\leq 1$|The closer to $1$ the better.|Only for hits in the ground truth. Rank-agnostic.                                                          |\n",
    "|nDCG|$\\geq 0$ and $\\leq 1$|The closer to $1$ the better.|Does not penalize for irrelevant/missing items in the ranking. For example, the following two recommended lists 1,1,1 and 1,1,1,0 would be considered equally good, even if the latter contains an irrelevant item. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f384d4",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "In this exercise, as before, you are reqired to write a number of functions. Only implemented functions are graded. Insert your implementations into the templates provided. Please don't change the templates even if they are not pretty. Don't forget to test your implementation for correctness and efficiency. **Make sure to try your implementations on toy examples and sanity checks.**\n",
    "\n",
    "Please **only use libraries already imported in the notebook**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fe8120b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T16:43:22.985329Z",
     "start_time": "2024-06-07T16:43:22.479643Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3383c3f4",
   "metadata": {},
   "source": [
    "## <font color='red'>TASK 1/3</font>: Predictive Quality Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b27f2c",
   "metadata": {},
   "source": [
    "### Precision@K\n",
    "\n",
    "Precision@k evaluates *how many items in the recommendation list are relevant* (hit) in the ground-truth data. Precision@K is calculated separately for every user and then averaged across all users. For each user, the precision score is normalized by **k**.\n",
    "\n",
    "It is defined as:\n",
    "\n",
    "$Precision@K = \\frac{1}{|Users|} \\sum_{u \\in Users} \\frac{|\\text{Relevant items}_u \\cap \\text{Recommended Items}_u(K)|}{K}$\n",
    "\n",
    "\n",
    "#### Input:\n",
    "* prediction - (**not** an interaction matrix!) numpy array with recommendations. Row index corresponds to ``user_id``, column index corresponds to the rank of the contained recommended item. Every cell (i,j) contains ``item id`` recommended to the user (i) on the position (j) in the list. For example:\n",
    "\n",
    "The following predictions ``[[12, 7, 99], [0, 97, 6]]`` mean that the user with ``id==1`` (second row) got recommended item **0** on the top of the list, item **97** on the second place and item **6** on the third place.\n",
    "\n",
    "* test_interaction_matrix - (plain interaction matrix, the same format as before!) interaction matrix built from interactions held out as a test set, rows - users, columns - items, cells - 0 or 1\n",
    "\n",
    "* topK - integer - top \"how many\" to consider for the evaluation. By default top 10 items are to be considered\n",
    "\n",
    "#### Output:\n",
    "* average ``Precision@k`` score across all users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de109f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pk_score(predictions: np.ndarray, test_interaction_matrix: np.ndarray, topK=10) -> float:\n",
    "    \"\"\"\n",
    "    predictions - np.ndarray, predictions of the recommendation algorithm for each user;\n",
    "    test_interaction_matrix - np.ndarray, test interaction matrix for each user;\n",
    "    topK - int, topK recommendations should be evaluated;\n",
    "    \n",
    "    returns - float, average precision@K score over all users;\n",
    "    \"\"\"\n",
    "    score = None\n",
    "    \n",
    "    # TODO: YOUR IMPLEMENTATION.\n",
    "    gen_sum=0\n",
    "    for u, inter_u in enumerate(test_interaction_matrix):\n",
    "        user_sum=0\n",
    "        relevant_idx = np.where(inter_u == 1)[0]\n",
    "        for i in relevant_idx:\n",
    "            if i in predictions[u,:topK]:\n",
    "                user_sum += 1        \n",
    "        gen_sum+=user_sum/topK\n",
    "        \n",
    "    n_users = predictions.shape[0]\n",
    "    score = gen_sum/(n_users)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c620ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([[0, 1, 2, 3], [3, 2, 1, 0]])\n",
    "test_interaction_matrix = np.array([[1, 0, 0, 0], [0, 0, 0, 1]])\n",
    "\n",
    "pk_score = get_pk_score(predictions, test_interaction_matrix, topK=4)\n",
    "\n",
    "assert np.isclose(pk_score, 0.25), \"precision@K score is incorrect.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e01193",
   "metadata": {},
   "source": [
    "### Recall@K\n",
    "\n",
    "Recall@k evaluates *how many relevant items in the ground-truth data are in the recommendation list*. Recall@K is calculated separately for every user and then averaged across all users. For each user, the recall score is normalized by the total number of ground-truth items.\n",
    "\n",
    "It is defined as:  \n",
    "\n",
    "$Precision@K = \\frac{1}{|Users|} \\sum_{u \\in Users} \\frac{|\\text{Relevant items}_u \\cap \\text{Recommended Items}_u(K)|}{|\\text{Relevant Items}_u|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b368e7d9",
   "metadata": {},
   "source": [
    "**Follow the \"same\" input and output defintion as for Precison@K**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0103a863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rk_score(predictions: np.ndarray, test_interaction_matrix: np.ndarray, topK=10) -> float:\n",
    "    \"\"\"\n",
    "    predictions - np.ndarray, predictions of the recommendation algorithm for each user;\n",
    "    test_interaction_matrix - np.ndarray, test interaction matrix for each user;\n",
    "    topK - int, topK recommendations should be evaluated;\n",
    "    \n",
    "    returns - float, average recall@K score over all users;\n",
    "    \"\"\"\n",
    "    score = 0.0\n",
    "    \n",
    "    # TODO: YOUR IMPLEMENTATION.\n",
    "    gen_sum=0\n",
    "    \n",
    "    for u, inter_u in enumerate(test_interaction_matrix):\n",
    "        user_sum=0\n",
    "        user_inter_sum= 0\n",
    "        relevant_idx = np.where(inter_u == 1)[0]\n",
    "        \n",
    "        for i in relevant_idx:\n",
    "            user_inter_sum+=1     \n",
    "            if i in predictions[u,:topK]:\n",
    "                user_sum += 1    \n",
    "                \n",
    "        gen_sum+=user_sum/user_inter_sum\n",
    "        \n",
    "    n_users = predictions.shape[0]\n",
    "    score = gen_sum/(n_users)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44fdf9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([[0, 1, 2, 3], [3, 2, 1, 0]])\n",
    "test_interaction_matrix = np.array([[1, 0, 0, 0], [0, 0, 0, 1]])\n",
    "\n",
    "rk_score = get_rk_score(predictions, test_interaction_matrix, topK=4)\n",
    "\n",
    "assert np.isclose(rk_score, 1), \"recall@K score is incorrect.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b044d9",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "* Assume a case, a user wants to find all good items. What is more important, high recall or high precision?\n",
    "* Write a one-sentence situation where high precision is more important than high recall.\n",
    "* How do recall and precision relate at Rth (Precision@R and Recall@R) position in the ranking (where R is the number of relevant items)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1812d0",
   "metadata": {},
   "source": [
    "*-- Answer Here --*\n",
    "* High recall\n",
    "* We're recommending something to clients who are well trained profesionals in their field, and they would drop us if we didn't recommend mostly relevant items (for recall e.g. our boss would drop us if we didn't recommend most of the something they want to push).\n",
    "* At R, recall is 1 if all relevant items are recommender, precision is the fraction of relevant items in the topR predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da35c1c9",
   "metadata": {},
   "source": [
    "## <font color='red'>TASK 2/3</font>: Ranking Quality Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9af194",
   "metadata": {},
   "source": [
    "Implement DCG and nDCG in the corresponding templates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6360ca",
   "metadata": {},
   "source": [
    "### DCG Score\n",
    "\n",
    "DCG@K measures the relevance of ranked items while giving higher importance to items appearing earlier in the ranking. It incorporates a logarithmic discount factor to penalize relevant items appearing lower in the ranking.\n",
    "\n",
    "nDCG@K is calculated separately for every user and then averaged across all users. It is defined as:  \n",
    "\n",
    "$DCG@K = \\sum^K_{i=1} \\frac{relevancy_i}{log_2(i+1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bccb23",
   "metadata": {},
   "source": [
    "**Follow the \"same\" input and output defintion as for Precison@K**.\n",
    "\n",
    "Don't forget, DCG is calculated for every user separately and then the average is returned.\n",
    "\n",
    "<font color='red'>**Attention!**</font> Use logarithm with base 2 for discounts! Remember that the top1 recommendation shouldn't get discounted! Users without interactions in the test set shouldn't contribute to the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d607a126",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T16:43:23.001289Z",
     "start_time": "2024-06-07T16:43:22.987324Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dcg_score(predictions: np.ndarray, test_interaction_matrix: np.ndarray, topK=10) -> float:\n",
    "    \"\"\"\n",
    "    predictions - np.ndarray - predictions of the recommendation algorithm for each user.\n",
    "    test_interaction_matrix - np.ndarray - test interaction matrix for each user.\n",
    "    \n",
    "    returns - float - mean dcg score over all user.\n",
    "    \"\"\"\n",
    "    score = None\n",
    "\n",
    "    # TODO: YOUR IMPLEMENTATION.\n",
    "    gen_sum=[]\n",
    "    \n",
    "    for u, preds_u in enumerate(predictions):\n",
    "        if np.sum(test_interaction_matrix[u]) == 0:\n",
    "            continue\n",
    "            \n",
    "        user_sum=0\n",
    "        for i, pred in enumerate(preds_u[:topK]): \n",
    "            if test_interaction_matrix[u, pred] ==1:\n",
    "                user_sum += 1 / np.log2(i + 2)\n",
    "                    \n",
    "        gen_sum.append(user_sum)\n",
    "\n",
    "    score = np.mean(gen_sum)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "376794a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T16:43:23.017247Z",
     "start_time": "2024-06-07T16:43:23.003283Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = np.array([[0, 1, 2, 3], [3, 2, 1, 0]])\n",
    "test_interaction_matrix = np.array([[1, 0, 0, 0], [0, 0, 0, 1]])\n",
    "\n",
    "dcg_score = get_dcg_score(predictions, test_interaction_matrix, topK=4)\n",
    "\n",
    "assert np.isclose(dcg_score, 1), \"1 expected\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86afdf65",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "* Can DCG score be higher than 1? Why?\n",
    "* Can the average DCG score be higher than 1? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3c0208",
   "metadata": {},
   "source": [
    "*-- Answer Here --*\n",
    "* Yes, in the sum we're adding up values that start at 1 and gradually get smaller and smaller (assuming the values exist, ie we have the relevant items), but generally there's no cap.\n",
    "* Yes, we average over users, who can have scores above 1, as in the previous bulletpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef924fee",
   "metadata": {},
   "source": [
    "### nDCG Score\n",
    "\n",
    "nDCG is a metric that evaluates how well the recommender performs in recommending ranked items to users. Therefore both hit of relevant items and correctness in ranking of these items matter to the nDCG evaluation. The total nDCG score is normalized by the total number of users.\n",
    "\n",
    "nDCG@K is calculated separately for every user and then averaged across all users. It is defined as:  \n",
    "\n",
    "$nDCG@K = \\frac{DCG@K}{iDCG@K}$\n",
    "\n",
    "**Follow the \"same\" input and output defintion as for Precison@K**\n",
    "\n",
    "<font color='red'>**Attention!**</font> Remember that ideal DCG is calculated separetely for each user and depends on the number of tracks held out for them as a Test set! Use logarithm with base 2 for discounts! Remember that the top1 recommendation shouldn't get discounted!\n",
    "\n",
    "<font color='red'>**Note:**</font> nDCG is calculated for **every user separately** and then the average is returned. You do not necessarily need to use the function you implemented above. Writing nDCG from scatch might be a good idea as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f2ec6a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T16:43:23.032208Z",
     "start_time": "2024-06-07T16:43:23.019242Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_ndcg_score(predictions: np.ndarray, test_interaction_matrix: np.ndarray, topK=10) -> float:\n",
    "    \"\"\"\n",
    "    predictions - np.ndarray - predictions of the recommendation algorithm for each user;\n",
    "    test_interaction_matrix - np.ndarray, test interaction matrix for each user;\n",
    "    topK - int, topK recommendations should be evaluated;\n",
    "    \n",
    "    returns - float, average ndcg score over all users;\n",
    "    \"\"\"   \n",
    "    score = 0\n",
    "    # TODO: YOUR IMPLEMENTATION.\n",
    "\n",
    "    gen_sum=[]\n",
    "\n",
    "    for u, preds_u in enumerate(predictions):\n",
    "        if np.sum(test_interaction_matrix[u]) == 0:\n",
    "            continue\n",
    "            \n",
    "        user_sum=0\n",
    "        for i, pred in enumerate(preds_u[:topK]): \n",
    "            if test_interaction_matrix[u, pred] ==1:\n",
    "                user_sum += 1 / np.log2(i + 2)\n",
    "\n",
    "        idcg = int(np.sum(test_interaction_matrix[u]))\n",
    "        idcg = idcg if idcg<topK else topK\n",
    "            \n",
    "        idcg_sum=0\n",
    "        for i in range(idcg):\n",
    "            idcg_sum+=1 / np.log2(i + 2)\n",
    "            \n",
    "        gen_sum.append(user_sum/idcg_sum)\n",
    "\n",
    "    score = np.mean(gen_sum)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4db41b05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T16:43:23.048167Z",
     "start_time": "2024-06-07T16:43:23.037197Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = np.array([[0, 1, 2, 3], [3, 2, 1, 0], [1, 2, 3, 0], [-1, -1, -1, -1]])\n",
    "test_interaction_matrix = np.array([[1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0]])\n",
    "\n",
    "ndcg_score = get_ndcg_score(predictions, test_interaction_matrix, topK=4)\n",
    "\n",
    "assert np.isclose(ndcg_score, 1), \"ndcg score is not correct.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34472609",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "* Can nDCG score be higher than 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a22178",
   "metadata": {},
   "source": [
    "*-- Answer Here --*\n",
    "* No, best case scenario we get DCG = iDCG, then we just get n=nr samples in the sum (add 1 for each sample), divide by n to average, we get 1. In suboptimal scenarios the proportion would be less than 1, so we'd get less than n in the average, i.e. (n-x)/n < 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22655bbf",
   "metadata": {},
   "source": [
    "## <font color='red'>TASK 3/3</font>: Evaluation\n",
    "Use the provided ``rec.py`` (see imports below) to build a simple evaluation framework. It should be able to evaluate ``POP``, ``ItemKNN`` and ``SVD``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "222a425e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T16:43:27.241289Z",
     "start_time": "2024-06-07T16:43:23.050163Z"
    }
   },
   "outputs": [],
   "source": [
    "from rec import inter_matr_implicit\n",
    "from rec import svd_decompose, svd_recommend_to_list  #SVD\n",
    "from rec import recTopK  #ItemKNN\n",
    "from rec import recTopKPop  #TopPop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "276fea39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T16:43:27.287169Z",
     "start_time": "2024-06-07T16:43:27.247273Z"
    }
   },
   "outputs": [],
   "source": [
    "def read(dataset, file):\n",
    "    return pd.read_csv(dataset + '/' + dataset + '.' + file, sep='\\t')\n",
    "\n",
    "\n",
    "users = read(\"lfm-tiny-tunes\", 'user')\n",
    "items = read(\"lfm-tiny-tunes\", 'item')\n",
    "train_inters = read(\"lfm-tiny-tunes\", 'inter_train')\n",
    "test_inters = read(\"lfm-tiny-tunes\", 'inter_test')\n",
    "\n",
    "train_interaction_matrix = inter_matr_implicit(users=users, items=items, interactions=train_inters,\n",
    "                                               dataset_name=\"lfm-tiny-tunes\")\n",
    "test_interaction_matrix = inter_matr_implicit(users=users, items=items, interactions=test_inters,\n",
    "                                              dataset_name=\"lfm-tiny-tunes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e414bd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Get Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52b7a07a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T16:43:27.319086Z",
     "start_time": "2024-06-07T16:43:27.289163Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_recommendations_for_algorithms(config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    config - dict - configuration as defined in the next cell\n",
    "\n",
    "    returns - dict - already predefined below with name \"rec_dict\"\n",
    "    \"\"\"\n",
    "\n",
    "    #use this structure to return results\n",
    "    rec_dict = {\"recommenders\": {\n",
    "        \"SVD\": {\n",
    "            #Add your predictions here\n",
    "            \"recommendations\": np.array([], dtype=np.int64)\n",
    "        },\n",
    "        \"ItemKNN\": {\n",
    "            \"recommendations\": np.array([], dtype=np.int64)\n",
    "        },\n",
    "        \"TopPop\": {\n",
    "            \"recommendations\": np.array([], dtype=np.int64)\n",
    "        },\n",
    "    }}\n",
    "\n",
    "    # TODO: YOUR IMPLEMENTATION.\n",
    "    train_inter = config[\"train_inter\"]\n",
    "    top_k = config[\"top_k\"]\n",
    "    n_factors = config[\"recommenders\"][\"SVD\"][\"n_factors\"]\n",
    "    n_neighbours = config[\"recommenders\"][\"ItemKNN\"][\"n_neighbours\"]\n",
    "    \n",
    "    U, V = svd_decompose(train_inter, n_factors)\n",
    "\n",
    "    n_users = train_inter.shape[0]\n",
    "\n",
    "    svd_recs = []\n",
    "    knn_recs = []\n",
    "    toppop_recs = []\n",
    "    for u in range(n_users):\n",
    "        seen = np.where(train_inter[u] > 0)[0].tolist()\n",
    "        _svd_recs = svd_recommend_to_list(u, seen, U, V, top_k)\n",
    "        svd_recs.append(_svd_recs)\n",
    "    \n",
    "        _knn_recs = recTopK(train_inter, user=u, top_k=top_k, n=n_neighbours)\n",
    "        knn_recs.append(_knn_recs)\n",
    "    \n",
    "        _toppop_recs = recTopKPop(train_inter, user=u, top_k=top_k)\n",
    "        toppop_recs.append(_toppop_recs)\n",
    "\n",
    "    rec_dict = {\"recommenders\": {\n",
    "        \"SVD\": {\n",
    "            #Add your predictions here\n",
    "            \"recommendations\": np.array(svd_recs, dtype=np.int64)\n",
    "        },\n",
    "        \"ItemKNN\": {\n",
    "            \"recommendations\": np.array(knn_recs, dtype=np.int64)\n",
    "        },\n",
    "        \"TopPop\": {\n",
    "            \"recommendations\": np.array(toppop_recs, dtype=np.int64)\n",
    "        },\n",
    "    }}\n",
    "\n",
    "    return rec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ae4eed7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T16:43:27.334048Z",
     "start_time": "2024-06-07T16:43:27.321080Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config_predict = {\n",
    "    #interaction matrix\n",
    "    \"train_inter\": train_interaction_matrix,\n",
    "    #topK parameter used for all algorithms\n",
    "    \"top_k\": 10,\n",
    "    #specific parameters for all algorithms\n",
    "    \"recommenders\": {\n",
    "        \"SVD\": {\n",
    "            \"n_factors\": 50\n",
    "        },\n",
    "        \"ItemKNN\": {\n",
    "            \"n_neighbours\": 5\n",
    "        },\n",
    "        \"TopPop\": {\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b78eab7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T16:46:04.655550Z",
     "start_time": "2024-06-07T16:43:27.336042Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "recommendations = get_recommendations_for_algorithms(config_predict)\n",
    "\n",
    "assert \"SVD\" in recommendations[\"recommenders\"] and \"recommendations\" in recommendations[\"recommenders\"][\"SVD\"]\n",
    "assert isinstance(recommendations[\"recommenders\"][\"SVD\"][\"recommendations\"], np.ndarray)\n",
    "assert np.issubdtype(recommendations[\"recommenders\"][\"SVD\"][\"recommendations\"].dtype, np.integer), \"Predictions must contain integer indices\"\n",
    "assert \"ItemKNN\" in recommendations[\"recommenders\"] and \"recommendations\" in recommendations[\"recommenders\"][\"ItemKNN\"]\n",
    "assert isinstance(recommendations[\"recommenders\"][\"ItemKNN\"][\"recommendations\"], np.ndarray)\n",
    "assert np.issubdtype(recommendations[\"recommenders\"][\"ItemKNN\"][\"recommendations\"].dtype, np.integer), \"Predictions must contain integer indices\"\n",
    "assert \"TopPop\" in recommendations[\"recommenders\"] and \"recommendations\" in recommendations[\"recommenders\"][\"TopPop\"]\n",
    "assert isinstance(recommendations[\"recommenders\"][\"TopPop\"][\"recommendations\"], np.ndarray)\n",
    "assert np.issubdtype(recommendations[\"recommenders\"][\"TopPop\"][\"recommendations\"].dtype, np.integer), \"Predictions must contain integer indices\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaf7a88",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluate Recommendations\n",
    "\n",
    "Implement the function such that it evaluates the previously generated recommendations. Make sure you use the provided config dictionary and pay attention to the structure for the output dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062c0fd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T16:50:20.890293Z",
     "start_time": "2024-06-07T16:50:20.872314Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_algorithms(config: dict, calculate_ndcg_score: Callable,\n",
    "    calculate_pk_score: Callable, calculate_rk_score: Callable,) -> dict:\n",
    "    \"\"\"\n",
    "    config - dict, configuration containing recommenders and test interaction matrix;\n",
    "    calculate_ndcg_score - callable, function to calculate the ndcg score;\n",
    "    calculate_pk_score - callable, function to calculate the precision@k score;\n",
    "    calculate_rk_score - callable, function to calculate the recall@k score;\n",
    "\n",
    "    returns - dict, { Recommender Key from input dict: \n",
    "        {\"ndcg\" : float \"ndcg score\"}\n",
    "        {\"pk\" : float \"precision@k score\"}\n",
    "        {\"rk\" : float \"recall@k score\"}\n",
    "    };\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {\n",
    "        \"SVD\": {\n",
    "            \"pk\": None,\n",
    "            \"rk\":None,\n",
    "            \"ndcg\": None,\n",
    "        },\n",
    "        \"ItemKNN\": {\n",
    "            \"pk\": None,\n",
    "            \"rk\":None,\n",
    "            \"ndcg\": None,\n",
    "        },\n",
    "        \"TopPop\": {\n",
    "            \"pk\": None,\n",
    "            \"rk\":None,\n",
    "            \"ndcg\": None,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # TODO: YOUR IMPLEMENTATION.\n",
    "    test_inter = config[\"test_inter\"]\n",
    "    top_k = config[\"top_k\"]\n",
    "\n",
    "    svd_recs = config[\"recommenders\"][\"SVD\"][\"recommendations\"]\n",
    "    knn_recs = config[\"recommenders\"][\"ItemKNN\"][\"recommendations\"]\n",
    "    toppop_recs = config[\"recommenders\"][\"TopPop\"][\"recommendations\"]\n",
    "    \n",
    "    svd_pk = calculate_pk_score(svd_recs, test_inter, top_k)\n",
    "    knn_pk = calculate_pk_score(knn_recs, test_inter, top_k)\n",
    "    toppop_pk = calculate_pk_score(toppop_recs, test_inter, top_k)\n",
    "\n",
    "    svd_rk = calculate_rk_score(svd_recs, test_inter, top_k)\n",
    "    knn_rk = calculate_rk_score(knn_recs, test_inter, top_k)\n",
    "    toppop_rk = calculate_rk_score(toppop_recs, test_inter, top_k)\n",
    "\n",
    "    svd_ndcg = calculate_ndcg_score(svd_recs, test_inter, top_k)\n",
    "    knn_ndcg = calculate_ndcg_score(knn_recs, test_inter, top_k)\n",
    "    toppop_ndcg = calculate_ndcg_score(toppop_recs, test_inter, top_k)\n",
    "\n",
    "    metrics = {\n",
    "        \"SVD\": {\n",
    "            \"pk\": svd_pk,\n",
    "            \"rk\":svd_rk,\n",
    "            \"ndcg\": svd_ndcg,\n",
    "        },\n",
    "        \"ItemKNN\": {\n",
    "            \"pk\": knn_pk,\n",
    "            \"rk\":knn_rk,\n",
    "            \"ndcg\": knn_ndcg,\n",
    "        },\n",
    "        \"TopPop\": {\n",
    "            \"pk\": toppop_pk,\n",
    "            \"rk\":toppop_rk,\n",
    "            \"ndcg\": toppop_ndcg,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdc1d63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T16:50:21.397607Z",
     "start_time": "2024-06-07T16:50:21.378645Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config_test = {\n",
    "    \"top_k\": 10,\n",
    "    \"test_inter\": test_interaction_matrix,\n",
    "    \"recommenders\": {}  # here you can access the recommendations from get_recommendations_for_algorithms\n",
    "\n",
    "}\n",
    "# add dictionary with recommendations to config dictionary\n",
    "config_test.update(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36d4b9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluating Every Algorithm\n",
    "Make sure everything works.\n",
    "We expect KNN to outperform other algorithms on our small data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f672c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T16:50:22.281330Z",
     "start_time": "2024-06-07T16:50:22.076856Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "evaluations = evaluate_algorithms(config_test, get_ndcg_score, get_pk_score, get_rk_score)\n",
    "\n",
    "evaluation_metrics = [\"pk\", \"rk\", \"ndcg\"]\n",
    "recommendation_algs = [\"SVD\", \"ItemKNN\", \"TopPop\"]\n",
    "\n",
    "for metric in evaluation_metrics:\n",
    "    for algorithm in recommendation_algs:\n",
    "        assert algorithm in evaluations and metric in evaluations[algorithm] and isinstance(evaluations[algorithm][metric], float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215bfaee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T16:50:22.828242Z",
     "start_time": "2024-06-07T16:50:22.814279Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for recommender in evaluations.keys():\n",
    "    print(f\"{recommender}:\")\n",
    "    print(f\"p@k: {evaluations[recommender]['pk']}\")\n",
    "    print(f\"r@k: {evaluations[recommender]['rk']}\")\n",
    "    print(f\"ndcg: {evaluations[recommender]['ndcg']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6191839c",
   "metadata": {},
   "source": [
    "## Questions and Potential Future Work\n",
    "* How would you try improve performance of all three algorithms?\n",
    "* What other metrics would you consider to compare these recommender systems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4ad182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T16:46:04.907876Z",
     "start_time": "2024-06-07T16:46:04.893913Z"
    }
   },
   "outputs": [],
   "source": [
    "# The end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vis-project)",
   "language": "python",
   "name": "vis-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
